# -*- coding: utf-8 -*-
"""U-Net.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VLSvuCTcEcKnd-GWBRP2d43TW1UByHBU
"""



"""# Image Segmentation with randomly genrated data"""

import numpy as np
import matplotlib.pyplot as plt

# generate a random image of size 10x10
image = np.random.random(size=(10,10))
print(image)
plt.figure(figsize=(5,5))
plt.imshow(image, cmap="gray", label="OG image")
plt.show()

# generate the mask for that image highlighting certain areas like

# import 0s for the same shape as og image
mask = np.zeros_like(image)
# highlight the masking region
mask[1:3, 6:8] = mask[5:9, 2:6] = 1
print(mask)
plt.imshow(mask, cmap="gray") # vmin=0, vmax=1 indicates lowest, hoghest possible value on color map
plt.show()

# Combining both
fig = plt.figure(figsize=(25, 50))  # Adjust the width and height as needed

plt.subplot(1,3,1)
plt.imshow(image, cmap="gray", label="OG image")
plt.title("OG")

plt.subplot(1,3,2)
plt.imshow(mask, cmap="gray", label="Mask")
plt.title("Mask")

plt.subplot(1,3,3)
plt.imshow(image, cmap="gray", label="OG image")
plt.imshow(mask, cmap="gray", alpha=0.5, label="Mask") # alpha: transperancy rate
plt.title("Combined")

plt.show()

"""
# U-Net Architecture Summary

## Overview
U-Net is a convolutional neural network architecture designed for semantic segmentation tasks. It is characterized by its symmetric encoder-decoder structure with skip connections that combine high-resolution features from the encoder with upsampled features in the decoder.

## Key Components

### 1. **Encoding Path (Contracting Path)**
   - **Function:** Extracts features and progressively downsamples the input image.
   - **Structure:**
     - Convolutional layers followed by MaxPooling layers.
     - Convolutional layers use increasing numbers of filters (e.g., 64, 128, 256) to capture more complex features.
   - **Purpose:** Learn hierarchical features while reducing spatial dimensions.

### 2. **Bottleneck**
   - **Function:** Processes features at the lowest resolution.
   - **Structure:**
     - Two convolutional layers with the highest number of filters (e.g., 256).
   - **Purpose:** Capture deep, complex features before upsampling.

### 3. **Decoding Path (Expansive Path)**
   - **Function:** Upsamples the feature maps to reconstruct the original image dimensions.
   - **Structure:**
     - UpSampling layers followed by concatenation with corresponding feature maps from the encoder.
     - Convolutional layers refine the upsampled features.
   - **Purpose:** Combine high-resolution features from the encoder to improve segmentation accuracy.

### 4. **Output Layer**
   - **Function:** Produces the final segmentation map.
   - **Structure:**
     - 1x1 Convolutional layer with a sigmoid activation function.
   - **Purpose:** Generate binary classification results for each pixel.

## Responses to Queries

1. **Increasing Number of Filters:**
   - **Reason:** Allows deeper layers to capture more complex and abstract features. Shallow layers detect simple features, while deeper layers handle more intricate patterns.

2. **Convolutions After Pooling:**
   - **Reason:** Refines and enhances high-level features, preserving and transforming information before upsampling for better accuracy in the decoding path.

3. **Concatenation in U-Net:**
   - **Reason:** Combines feature maps from the encoder with those from the decoder to retain detailed features and spatial information lost during downsampling.

4. **Feature Maps and Kernels:**
   - **Result:** Applying \( M \) kernels to \( N \) feature maps results in \( M \) new feature maps, not \( M \times N \).

5. **Concatenation and Vanishing Gradient:**
   - **Reason:** Helps mitigate the vanishing gradient problem by improving gradient flow through the network, facilitating effective training.

6. **Concatenation and Exploding Gradient:**
   - **Reason:** Concatenation does not directly address exploding gradients; techniques like gradient clipping and normalization are used for that. Concatenation primarily helps with vanishing gradients.
"""

import os
import cv2

dataset_directory = "/content/drive/MyDrive/Segmentation" # home dir
folders = os.listdir(dataset_directory)
print(folders)
images = {}
masks = {}
for folder in folders:
  folder_path = os.path.join(dataset_directory, folder)
  if os.path.isdir(folder_path): # Images directory
    for file in os.listdir(folder_path):
      name = file[:-4]
      if file.endswith(".jpg"):
        image_path = os.path.join(folder_path, file)
        image = cv2.imread(image_path)
        images[name] = image
      if file.endswith(".png"):
        mask_path = os.path.join(folder_path, file)
        mask = cv2.imread(mask_path)
        masks[name] = mask
print(len(images), len(masks))

sorted(images.keys())==sorted(masks.keys())

def sort_images(d):
  d = dict(sorted(d.items()))
  return list(d.values())
image_list = sort_images(images)
mask_list = sort_images(masks)

print(len(image_list), len(mask_list))

# Check the types of images in image_list and mask_list
image_types = set()
mask_types = set()
for i in image_list:
  image_types.add(type(i))
for i in mask_list:
  mask_types.add(type(i))
print(image_types, mask_types)

# get indices where the value is None
none_indices = [i for i in range(len(image_list)) if image_list[i] is None]
print(none_indices)

for i in reversed(none_indices): # reverse to avoid index shifts
  image_list.pop(i)
  mask_list.pop(i)

image_types = set()
mask_types = set()
for i in image_list:
  image_types.add(type(i))
for i in mask_list:
  mask_types.add(type(i))
print(image_types, mask_types)

# To check if the image and mask are mapped
import matplotlib.pyplot as plt
import numpy as np

print(len(image_list), len(mask_list))


for i in range(3):
  trail_num = np.random.randint(len(image_list))
  plt.subplot(1,2,1)
  plt.imshow(image_list[trail_num])
  plt.subplot(1,2,2)
  plt.imshow(mask_list[trail_num])
  plt.show()

# Check size and Resize if necessary

image_size_set = set()
mask_size_set = set()
for i in image_list:
  image_size_set.add(i.shape)
for i in mask_list:
  mask_size_set.add(i.shape)
print(image_size_set, "\n",mask_size_set)

import tensorflow as tf
# Wrong
# for i in image_list:
#   i = tf.image.resize(i, (256,256))
# for i in mask_list:
#   i = tf.image.resize(i, (256,256))
# Right
image_list_resized = [tf.image.resize(i, (256,256)) for i in image_list]
mask_list_resized = [tf.image.resize(i, (256,256)) for i in mask_list]

image_size_set = set()
mask_size_set = set()
for i in image_list_resized:
  image_size_set.add(i.shape)
for i in mask_list_resized:
  mask_size_set.add(i.shape)
print(image_size_set, "\n",mask_size_set)

# Note
print(type(image_list[0]), type(image_list_resized[0]))

# Note if you want the numpy reshapes only
# a) use cv2:
# image_list_resized = [cv2.resize(i, (256,256)) for i in image_list]
# b) convert tf to numpy with code:
# image_list_resized = [i.numpy() for i in image_list_resized]

# To check the scale and normalize if needed
import numpy as np

image_set = set()
mask_set = set()
for i in image_list:
  image_set.add(np.min(i))
  image_set.add(np.max(i))
for i in mask_list:
  mask_set.add(np.min(i))
  mask_set.add(np.max(i))
print(min(image_set), max(image_set), "\n",min(mask_set), max(mask_set))

# Above output shows the requirement for normalization. So:

# Method-1: Doing resizing on pyhton-lists - SESSION CRASH!
"""image_list_normalized = [i/255 for i in image_list_resized]
mask_list_normalized = [i/255 for i in mask_list_resized]"""
# Method-2: Doing resizing by converting teh lists to numpy arrays: That is why we mostly work on numpy as it performs the ops element wise
# Note resizing all images to same shape is a must to convert to numpy
image_list_np = np.array(image_list_resized)
mask_list_np = np.array(mask_list_resized)
image_list_normalized = image_list_np/255
mask_list_normalized = mask_list_np/255

image_set = set()
mask_set = set()
image_set.add(np.min(image_list_normalized))
image_set.add(np.max(image_list_normalized))
mask_set.add(np.min(mask_list_normalized))
mask_set.add(np.max(mask_list_normalized))
print(image_set, mask_set)

print(image_list_normalized.shape)
print(mask_list_normalized.shape)

# Since we need the masked images in grayscale, c=1 for mask
mask_list_normalized_grayed = [tf.image.rgb_to_grayscale(i) for i in mask_list_normalized]
mask_list_normalized_grayed = np.array(mask_list_normalized_grayed)

print(image_list_normalized.shape)
print(mask_list_normalized_grayed.shape)

import tensorflow as tf
import numpy as np

# Define augmentation functions
def i1(image):
    return tf.image.random_flip_left_right(image)

def i2(image):
    return tf.image.random_flip_up_down(image)

def a3(image):
    return tf.image.random_brightness(image, max_delta=0.1)

def a4(image):
    return tf.image.random_contrast(image, lower=0.9, upper=1.1)

# Convert numpy arrays to TensorFlow tensors
image_list_tf = tf.convert_to_tensor(image_list_normalized, dtype=tf.float32)
mask_list_tf = tf.convert_to_tensor(mask_list_normalized_grayed, dtype=tf.float32)

# Define a dataset from tensors
dataset = tf.data.Dataset.from_tensor_slices((image_list_tf, mask_list_tf))

# Apply augmentations and create datasets
batch_size = 8

def create_augmented_dataset(augment_func):
    augmented_dataset = dataset.map(lambda img, msk: (augment_func(img), augment_func(msk)))
    return augmented_dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)

datasets = {
    "original": dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE),
    "i1": create_augmented_dataset(i1),
    "i2": create_augmented_dataset(i2),
    "a3": create_augmented_dataset(a3),
    "a4": create_augmented_dataset(a4)
}

# Convert augmented dataset to NumPy arrays
def dataset_to_numpy(dataset, num_samples):
    images = []
    masks = []
    for image_batch, mask_batch in dataset.take(num_samples // batch_size):
        images.append(image_batch.numpy())
        masks.append(mask_batch.numpy())
    return np.concatenate(images, axis=0), np.concatenate(masks, axis=0)

# Number of samples to generate in the final dataset
num_samples_per_dataset = len(image_list_normalized)  # Example for one dataset
num_total_samples = num_samples_per_dataset * (len(datasets) + 1)  # Original + augmented

# Convert each dataset to NumPy arrays
numpy_datasets = {}
for key, ds in datasets.items():
    numpy_images, numpy_masks = dataset_to_numpy(ds, num_samples_per_dataset)
    numpy_datasets[key] = (numpy_images, numpy_masks)

# Also convert the original dataset
original_images, original_masks = dataset_to_numpy(datasets["original"], num_samples_per_dataset)

# Combine original and augmented datasets
combined_images = np.concatenate([original_images] + [numpy_datasets[key][0] for key in datasets if key != "original"], axis=0)
combined_masks = np.concatenate([original_masks] + [numpy_datasets[key][1] for key in datasets if key != "original"], axis=0)

print("Combined images shape:", combined_images.shape)
print("Combined masks shape:", combined_masks.shape)

# Model for U-Net
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate

# padding = "same" to have the same shape of image afer convolution
def unet_model(input_shape):
  inputs = Input(shape=input_shape)

  c1 = Conv2D(64, (3,3), activation="relu", padding='same')(inputs)
  p1 = MaxPooling2D((2,2))(c1)

  bottleneck = Conv2D(128, (3,3), activation="relu", padding='same')(p1)
  bottleneck = Conv2D(128, (3,3), activation="relu", padding='same')(bottleneck)

  u2 = UpSampling2D((2,2))(bottleneck)
  u2 = Concatenate()([u2, c1])
  c4 = Conv2D(64, (3,3), activation="relu", padding='same')(u2)

  # Final layer, since here we are not doing classification we no need to Flatten(), output is also a the same image of size
  """
  The comment is correct in stating that you do not need to flatten the output in U-Net, as the output is designed to be the same size as the input image. The final convolutional layer with a 1Ã—1 kernel maintains the spatial dimensions and adjusts the number of channels to match the desired output (e.g., binary masks in segmentation tasks).
  """
  output_layer = Conv2D(1, (1,1), activation="sigmoid")(c4)

  model = tf.keras.Model(inputs=inputs, outputs=output_layer)
  return model

model = unet_model((256,256,3))
# model.build((None, 256,256,3))# input_shape passed, so (h,w,c)
model.summary()

from tensorflow.keras.optimizers import Adam
optimizer = Adam(learning_rate=0.001)
loss = "binary_crossentropy"
model.compile(optimizer=optimizer, loss=loss, metrics=["accuracy"])

# Converting numpy arrays to tensors
import tensorflow as tf

image_list_normalized_tf = tf.convert_to_tensor(image_list_normalized, dtype=tf.float32)
mask_list_normalized_grayed_tf = tf.convert_to_tensor(mask_list_normalized_grayed, dtype=tf.float32)


history = model.fit(image_list_normalized, mask_list_normalized_grayed, epochs=1, batch_size=8, verbose=2)

import cv2
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

def preprocess_and_predict(image_path, model, input_size=(256, 256), threshold=0.5):
    """
    Preprocess the image and use the trained model to make predictions.

    Parameters:
    - image_path (str): Path to the input image.
    - model (tf.keras.Model): Trained TensorFlow/Keras model.
    - input_size (tuple): Size to which the image should be resized (height, width).
    - threshold (float): Threshold for binary mask prediction.

    Returns:
    - predicted_mask (numpy array): The predicted mask from the model.
    """
    # Load the image
    image = cv2.imread(image_path)

    # Check if image is loaded correctly
    if image is None:
        raise FileNotFoundError(f"Image not found at path: {image_path}")

    # Resize the image to the required input size of the model
    image_resized = cv2.resize(image, input_size)

    # Normalize the image
    image_normalized = image_resized / 255.0

    # Add a batch dimension
    image_batch = np.expand_dims(image_normalized, axis=0)

    # Make predictions
    predictions = model.predict(image_batch)

    # Post-process the output (assuming binary mask for simplicity)
    predicted_mask = predictions[0]  # Remove the batch dimension
    predicted_mask = (predicted_mask > threshold).astype(np.uint8)  # Apply threshold if needed

    return image_resized, predicted_mask

# Example usage
# Assuming `model` is your trained U-Net model
image_path = "/content/drive/MyDrive/Segmentation/Image/10.jpg"
image_resized, predicted_mask = preprocess_and_predict(image_path, model)

# Visualize the results
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.title("Original Image")
plt.imshow(cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB))
plt.axis('off')

plt.subplot(1, 2, 2)
plt.title("Predicted Mask")
plt.imshow(predicted_mask.squeeze(), cmap='gray')  # Squeeze to remove single-dimensional entries
plt.axis('off')

plt.show()

# Debug tries
# pip install --upgrade tensorflow

# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=1)a
# history = model.fit(image_list_normalized_tf, mask_list_normalized_grayed_tf, epochs=3, batch_size=8, verbose=2, callbacks=[tensorboard_callback])

# To batch the data, so as to give the model to model.fit()

# WRONG
# image_list_batched = [tf.expand_dims(i, axis=0) for i in image_list_resized]
# mask_list_batched = [tf.expand_dims(i, axis=0) for i in mask_list_resized]
# image_size_set = set()
# mask_size_set = set()
# for i in image_list_batched:
#   image_size_set.add(i.shape)
# for i in mask_list_batched:
#   mask_size_set.add(i.shape)
# print(image_size_set, "\n",mask_size_set)

# RIGHT
image_list_batched = tf.data.Dataset.from_tensor_slices(image_list_normalized)
mask_list_batched = tf.data.Dataset.from_tensor_slices(mask_list_normalized)
image_list_batched = image_list_batched.batch(1)
mask_list_batched = mask_list_batched.batch(1)
for i in image_list_batched:
  print(i.shape)
for i in mask_list_batched:
  print(i.shape)

"""

# Understanding the existing shapes
for image in image_list_resized:
  print(image.shape)
for mask in mask_list_resized:
  print(mask.shape)

# ???
image_tensors = tf.data.Dataset.from_tensor_slices(image_list_resized)
mask_tensors = tf.data.Dataset.from_tensor_slices(mask_list_resized)
print("Lenghths: ",len(image_tensors), len(mask_tensors))
for image in image_tensors.take(1):
    print(image.shape)
for mask in mask_tensors.take(1):
    print(mask.shape)

# adding a bacth size to each image
image_tensors_batched = image_tensors.batch(1)
mask_tensors_batched = mask_tensors.batch(1)
for image in image_tensors_batched.take(1):
    print(image.shape)
for mask in mask_tensors_batched.take(1):
    print(mask.shape)


combined_dataset = tf.data.Dataset.zip((image_tensors_batched, mask_tensors_batched))
"""

# print("Images type: ",type(image_list_normalized))
# print("Images Len: ",image_list_normalized.shape)

# print("Mask type: ", type(mask_list_normalized_grayed))
# print("Mask Len: ", mask_list_normalized_grayed.shape)

# print("TensorFlow version:", tf.__version__)

# # To check if the image and mask are still mapped
# import matplotlib.pyplot as plt
# import numpy as np

# print(len(image_list), len(mask_list))


# for i in range(3):
#   trail_num = np.random.randint(len(image_list))
#   plt.subplot(1,2,1)
#   plt.imshow(image_list_normalized[trail_num])
#   print(np.min(image_list_normalized[trail_num]), np.max(image_list_normalized[trail_num]))
#   plt.subplot(1,2,2)
#   plt.imshow(mask_list_normalized_grayed[trail_num], cmap="gray")
#   print(np.min(mask_list_normalized_grayed[trail_num]), np.max(mask_list_normalized_grayed[trail_num]))
#   plt.show()